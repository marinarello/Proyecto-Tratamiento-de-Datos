<div align="center">
  
  **Proyecto Final** 
  
  **Tratamiento de Datos**
  
  **M치ster de Ing. de Telecomunicaci칩n**

  **Daniel Mu침oz y Marina Rello**
  
</div>

El proyecto b치sico consistir치 en la resoluci칩n de una tarea de regresi칩n, comparando las prestaciones obtenidas al utilizar distintas vectorizaciones de los documentos y al menos dos estrategias distintas de aprendizaje autom치tico, seg칰n se describe a continuaci칩n. Los pasos que debe seguir en su trabajo son los siguientes:

Como paso inicial, observamos el dataset con el que se va a trabajar:
<div align="center">
  <img src="images/Dataset_completo.png" alt="Gr치fica 1">
</div>

Se observan valores vac칤os en el dataset, por lo que se realiza una limpieza del mismo eliminando estos valores vac칤os.

Una vez eliminados los valores vac칤os del dataset, observamos los valores num칠ricos del dataset, con el fin de entender mejor la informaci칩n contenida:
<div align="center">
  <img src="images/outliers.png" alt="Gr치fica 1">
</div>

En cada histograma, los valores en el eje horizontal son extremadamente grandes, pero la mayor parte de los datos se concentran cerca de un rango m치s peque침o (cercano a cero). Esto sugiere que hay valores muy grandes (outliers) que "alargan" el eje y distorsionan la visualizaci칩n de la distribuci칩n principal. 

Se ha realizado una limpieza de dichos "outliers", tras la limpieza, volvemos a observamos los valores num칠ricos del dataset. Los valores extremos (outliers) que antes estiraban las escalas de los ejes han sido eliminados. Ahora las distribuciones muestran de forma m치s clara y representativa c칩mo se concentran los datos:

<div align="center">
  <img src="images/sin_outliers.png" alt="Gr치fica 1">
</div>


# 1. An치lisis de variables de entrada. Visualice la relaci칩n entre la variable de salida y algunas de las categor칤as en la variable categories y explique su potencial relevancia en el problema.

Como paso previo, se ha estudiado la relaci칩n de las distintas variables num칠ricas con el "rating":
<div align="center">
  <img src="images/numericas.png" alt="Gr치fica 1">
</div>

Se han realizado gr치ficos de dispersi칩n para analizar la relaci칩n entre las variables num칠ricas (fat, calories, protein y sodium) y el rating. Al observar los resultados, se puede concluir que no existe una relaci칩n clara o significativa, ya que los puntos se encuentran dispersos y no muestran patrones definidos. La variabilidad del "rating" se mantiene amplia para todos los valores de las variables num칠ricas, lo que indica que estas no tienen un impacto directo en el comportamiento del rating.


En el an치lisis de las variables de entrada, se ha explorado la relaci칩n entre la variable de salida rating y algunas categor칤as de la columna categories.
Se han ido probando distintas categor칤as de la variable categories para analizar su relaci칩n con la variable de salida rating, encontrando resultados contrastantes. Por ejemplo, las categor칤as "Pasta" y "Beef" muestran una mayor concentraci칩n de ratings en valores altos, especialmente entre 4 y 5, lo que sugiere una ligera relaci칩n positiva con el rating, ya que las recetas pertenecientes a estas categor칤as tienden a ser mejor valoradas. En cambio, otras categor칤as como "Alcoholic" y "Drink" presentan una distribuci칩n de ratings mucho m치s dispersa, con valores repartidos en todo el rango, lo que indica que no tienen una relaci칩n clara con la variable de salida.

<div align="center">
  <img src="images/categories_vs_rating.png" alt="Gr치fica 1">
</div>


# 2. Implementaci칩n de un pipeline para el preprocesado de los textos. Para esta tarea puede usar las librer칤as habituales (NLTK, Gensim o SpaCy), o cualquier otra librer칤a que considere oportuna. Tenga en cuenta que para trabajar con transformers el texto se pasa sin preprocesar.

En este paso se han transformado los datos de entrada de texto en bruto en una representaci칩n vectorial. Para ello, se ha eliminado la informaci칩n irrelevante de los datos de texto, preservando la mayor cantidad de informaci칩n relevante posible para capturar el contenido sem치ntico en la colecci칩n de documentos.
Para ello se han realizado los siguientes pasos:
  - Tokenization: Se ha dividido el texto en unidades m치s peque침as llamadas tokens, para poder trabajar con cada elemento del texto de manera independiente.
  - Homogeneization: Se estandariza el texto para reducir variaciones innecesarias, como convertir todo a min칰sculas, eliminar acentos y elementos no alfanum칠ricos o normalizar t칠rminos similares.
  - Cleaning: se han eliminado aquellas palabras que son muy comunes en el idioma y no aportan contenido sem치ntico 칰til 
  - Vectorization: Se ha transformado el texto procesado en una representaci칩n num칠rica (vectores) que los algoritmos pueden interpretar. Estos vectores capturan la informaci칩n sem치ntica y estructural del texto. Para ello, se ha creado un diccionario que asocia cada token con un identificador 칰nico y se han eliminado palabras que aparecen en muy pocos documentos o en demasiados. Cada documento se convierte en una lista de tuplas incluyendo el identificador 칰nico del token y la cantidad de veces que ese token aparece en el documento. Esto produce una representaci칩n dispersa (sparse vector), donde las palabras relevantes del texto est치n asociadas con su frecuencia. Finalmente, cada documento se representa como un vector disperso, donde los identificadores de los tokens corresponden a posiciones espec칤ficas del vector, y los valores representan la frecuencia.

Por 칰ltimo vamos a crear un diccionario para cada una de las columnas ya preprocesadas, y una vez creado realizaremos una limpieza para eliminar las palabras del diccionario que aparezcan en mas del 80% de recetas y en menos de 5 documentos.Terminaremos realizando un Bag of Words (BoW) para las columnas preprocesadas del dataframe, de esta manera habremos realizado una vectorizaci칩n simple de nustro texto.

# 3. Representaci칩n vectorial de los documentos mediante tres procedimientos diferentes:
Una vez conseguido nustras columnas de texto ya preprocesadas en el paso anterior, podremos proceder con la vectoriazion de estas columnas con distintos modelos.
## - TF-IDF
Comenzaremos con la vectorizaci칩n mediante TF-IDF. Para ello empleamos el modelo TfidfVectorizer el caul aprende el vocabulario y su importancia mediante un fit, y a continuaci칩n realiza una transformacion de los textos en vectores mediante un transform. Obtedremos una matriz en la que veremos la representacion de cada palabra de una receta con su puntuaje de TF-IDF.
Realizamos estos pasos con cada una de las columnas de texto que tenemos.

## - Word2Vec(es decir, la representaci칩n de los documentos como promedio de los embeddings de las palabras que lo forman)
Para la vectorizacion de las columnas de texto empleando Word2Vec que consiste en caprturar las relaciones sem치nticas y sint치cticas entre palabras.
En primer lugar deberemos de entrenar nuestro modelo sobre la columna de texto ya preprocesada a vectorizar. Al entrenar el modelo hemos empleado los siguientes parametros:
min_count=1 para incluir las palabras que aparecen al menos una vez, vector_size=100 para que cada palabra sea representada mediante un vector de 100 dimensiones y por 칰ltimo workers=4 donde indicamos el n칰mero de hilos a emplear para el entrenamiento del modelo.
Finalmente a침adimos a nuestro dataframe en unas columnas nuevas los vectores obtenidos mediante la vectorizaci칩n de W2V.

<div align="center">
  <img src="images/w2v.jpg" alt="Gr치fica 1">
</div>


## - Embeddings contextuales calculados a partir de modelos basados en transformers (e.g., BERT, RoBERTa, etc).
Por 칰ltimo realizaremos la vectorizaci칩n de las columnas de texto de nuestro dataframe empleando un modelo preentrenado de BERT ('bert-base-uncased') para generar los embeddings, colocaremos el el modelo en modo evaluacion para desactivas su entrenamiento y evitar asi cambios en los pesos.
En primer lugar deberemos de recorrer cada fila de las columnas de texto y colocar las etiquetas "CLS" y "SEP" al inicio y al final de cada frase que tengamos, para el correcto funcionamiento de nuestro modelo. Hay que tener en cuenta que nuestrotexto tkenizado no puede tener un tama침o superior a 512 por lo que trataremos nuestros datos para que esto no llegue a ocurrir.
Una vez tokenizado nuestro c칩digo mapearemos los tokens a 칤ndices del vocabulario de BERT.
Para finalizar con la vectorizaci칩n, crearemos los tensores para los tokens y para los segmentos que se utilizaran en BERT para identificar si los tokens pertenecen a una o dos frases diferentes.
Finalmente a침adiremos al dataframe en nuevas columnas los vectores obtenidos para cada receta.


# 4. Entrenamiento y evaluaci칩n de modelos de regresi칩n utilizando al menos las dos estrategias siguientes de aprendizaje autom치tico:
Cada modelo de regresi칩n ha sido entrenado y evaluado utilizando las tres t칠cnicas de vectorizaci칩n presentadas en el apartado anterior, con el objetivo de comparar los resultados obtenidos y determinar cu치l de las t칠cnicas ofrece el mejor rendimiento.

En primer lugar, se llev칩 a cabo una regresi칩n utilizando el modelo k-NN con la vectorizaci칩n Word2Vec. Para ello, se prob칩 el modelo empleando diferentes combinaciones de columnas como entrada (洧녦), con el prop칩sito de identificar cu치les ofrec칤an los mejores resultados. Una vez determinada la combinaci칩n 칩ptima, esta se utiliz칩 de manera uniforme en todos los modelos para garantizar la coherencia en la comparaci칩n.

La tabla que se presenta a continuaci칩n muestra las distintas combinaciones de columnas evaluadas y los valores de error cuadr치tico medio (MSE) obtenidos en cada caso.

<div align="center">

| **Combinaci칩n de Columnas**                                                  | **MSE (k-NN)** |
|------------------------------------------------------------------------------|---------------:|
| directions_W2V, otras_columnas, categories_W2V, desc_W2V, title_W2V, ingredients_W2V | 1.55          |
| directions_W2V, otras_columnas, categories_W2V, desc_W2V                     | 1.55          |
| directions_W2V, otras_columnas, categories_W2V, title_W2V                    | 1.55          |
| directions_W2V, otras_columnas, categories_W2V                               | 1.55          |
| directions_W2V, categories_W2V, title_W2V                                    | 1.49          |
| directions_W2V, categories_W2V, desc_W2V, title_W2V, ingredients_W2V         | 1.49          |
| directions_W2V, categories_W2V, desc_W2V, title_W2V                          | 1.47          |

</div>




Durante este an치lisis, se observ칩 que las combinaciones que inclu칤an columnas de valores num칠ricos produc칤an peores resultados, por lo que dichas columnas fueron excluidas de la entrada del modelo. Finalmente, la combinaci칩n que ofreci칩 los mejores resultados incluye las columnas: directions, categories, desc y title.

## - Redes neuronales utilizando PyTorch para su implementaci칩n.
Se ha implementado una red neuronal dise침ada con dos capas ocultas: la primera con 50 neuronas y la segunda con 25, ambas utilizando la funci칩n de activaci칩n ReLU, y una capa de salida con una 칰nica neurona para predecir valores continuos. Se divide el conjunto de datos en entrenamiento (80%) y prueba (20%), convirtiendo los datos a tensores para su uso en PyTorch. La red se entrena durante 25 칠pocas usando el optimizador Adam y la funci칩n de p칠rdida de error cuadr치tico medio (MSELoss), procesando los datos en lotes de 32 muestras con un DataLoader para mejorar la eficiencia. Finalmente, se eval칰a el modelo en los datos de prueba calculando la p칠rdida y el coeficiente R2.

## - Al menos otra t칠cnica implementada en la librer칤a Scikit-learn (e.g., K-NN, SVM, Random Forest, etc)
Se han implementado los modelos de regresi칩n k-NN y Random Forest. 
Para el modelo k-NN, se ha realizado validaci칩n cruzada en cada vectorizaci칩n con el objetivo de determinar el valor 칩ptimo del hiperpar치metro 洧녲. 
En el caso de Random Forest, debido a las limitaciones de tiempo y al elevado costo computacional, el ajuste de los hiperpar치metros "n_estimators" y "max_depth" se ha realizado 칰nicamente para una de las vectorizaciones, utilizando los valores obtenidos como referencia para el resto de los modelos.

A continuaci칩n, se presenta una tabla comparativa que recoge los resultados obtenidos para cada modelo de regresi칩n aplicado a las diferentes t칠cnicas de vectorizaci칩n.

<div align="center">
<table>
    <tr>
        <th>Modelo</th>
        <th>Vectorizaci칩n</th>
        <th>MSE</th>
        <th>MAE</th>
        <th>R</th>
        <th>RMSE</th>
    </tr>
    <tr>
        <td rowspan="3">k-NN</td>
        <td>TF-IDF</td>
        <td>1.436</td>
        <td>0.807</td>
        <td>1.198</td>
        <td>0.069</td>
    </tr>
    <tr>
        <td>Word2Vec</td>
        <td>1.518</td>
        <td>0.822</td>
        <td>1.232</td>
        <td>0.016</td>
    </tr>
    <tr>
        <td>BERT</td>
        <td>1.518</td>
        <td>0.822</td>
        <td>1.232</td>
        <td>0.016</td>
    </tr>
    <tr>
        <td rowspan="3">Random Forest</td>
        <td>TF-IDF</td>
        <td>1.387</td>
        <td>0.794</td>
        <td>1.178</td>
        <td>0.101</td>
    </tr>
    <tr>
        <td>Word2Vec</td>
        <td>1.413</td>
        <td>0.801</td>
        <td>1.189</td>
        <td>0.084</td>
    </tr>
    <tr>
        <td>BERT</td>
        <td>1.61</td>
        <td>0.83</td>
        <td>0.044</td>
        <td>1.27</td>
    </tr>
    <tr>
        <td rowspan="3">Red Neuronal</td>
        <td>TF-IDF</td>
        <td>1.524</td>
        <td>X</td>
        <td>0.043</td>
        <td>X</td>
    </tr>
    <tr>
        <td>Word2Vec</td>
        <td>1.585</td>
        <td>X</td>
        <td>0.004</td>
        <td>X</td>
    </tr>
    <tr>
        <td>BERT</td>
        <td>1.67</td>
        <td>X</td>
        <td>-0.05</td>
        <td>X</td>
    </tr>
</table>

</div>

Observamos que la vectorizaci칩n que mejores resultados obtiene en los tres modelos de regresion es TF-IDF, y que el mejor resultado se obtiene con Random Forest.


# 5. Comparaci칩n de lo obtenido en el paso 3 con el fine-tuning de un modelo preentrenado con Hugging Face. En este paso se pide utilizar un modelo de tipo transformer con una cabeza dedicada a la tarea de regresi칩n.
En este punto entrenaremos y evaluaremos un modelo BERT.
Para el entrenamiento hemos establecido la columna de descripciones como entrada y la columna de ratings como salida. Comenzaremos dividiendo los datos en un conjunto de entrenamiento(80%) y test(20%). 
Al igual que en el punto 3 empleamos el modelo y el tokenizador de 'bert-base-uncased', y a continuaci칩n realizamos una conversi칩n de las listas de texto creadas anteriormente en tokens de BERT, teniendo en cuenta algunas medidas como la longitud maxima de este token y el relleno de estos para que todos sean de la misma longitud.

De cara al entrenamiento del modelo primeramente configuramos los argumentos a emplear para el entrenamiento, como por ejemplo directorio donde guardaremos los resultados del modelo,el n칰mero de epochs a emplear (1), o tambi칠n la frecuencia de registro de m칠tricas.
Por ultimo realizaremos el entrenamiento y la evaluaci칩n del modelo y nos lo guardaremos en local para su posterior uso.

Una vez ya tenemos nuestro modelo entrenado y evaluado,podemos seguir los mismos pasos que en el apartado 3, para realizar la vectorizacion medainte BERT de nuestras columnas de texto. El 칰nico cambio que deberemos de realizar es cambiar el tokenizer y el modelo y sustituir el que empleabamos en el apartado 3 por nuestro nuevo modelo entrenado.

Para la comparaci칩n de resultados entre los dos modelos de BERT realizaremos los mismos pasos con ambos modelos. Analizaremos los resultados del MSE, MAE, RMSE y R^2 de la  red neuronal, y los modelos de regrei칩n KNN y Random Forest.
Los resultados obtrenidos se pueden apreciar en la siguiente tabla:
<div align="center">

| Modelo BERT       | Modelo de Regresi칩n |    MSE    |    MAE    |   RMSE   |     R    |
|:-----------------:|:-------------------:|:---------:|:---------:|:--------:|:---------:|
| BERT-Modelo 1     | KNN                |   1.51   |   0.822   |  1.23   |   0.016    |
|                   | Random Forest      |  1.61   |   0.83   |  1.27  |   0.044    |
|                   | Red Neuronal       |   1.67   |   X   | X   |   -0.05    |
| BERT-Modelo 2     | KNN                |   1.48   |   0.77   |  1.22   |   0.04    |
|                   | Random Forest      |   1.39   |   0.79   |  1.18   |   0.09    |
|                   | Red Neuronal       |   1.52   |   X   |  X  |  0.04    |

</div>

Se puede apreciar que con nuestro nuevo modelo hemos conseguido unos resultados ligeramente mejores.

# EXTENSI칍N
En la extensi칩n se ha realizado una tarea de traducci칩n de texto.
Se ha utilizado la librer칤a Hugging Face Transformers para traducir autom치ticamente los t칤tulos de la columna title del ingl칠s al espa침ol, empleando el modelo pre-entrenado Helsinki-NLP/opus-mt-en-es. Primero, se configura un pipeline para realizar la tarea de generaci칩n de texto orientada a la traducci칩n. Luego, se recorre cada t칤tulo en la columna y se pasa al modelo un comando expl칤cito para traducir, como "translate from English to Spanish: {title}". El modelo genera el texto traducido, que se extrae y almacena en una lista. Finalmente, esta lista de traducciones se agrega como una nueva columna, title_translated, en el DataFrame original, permitiendo tener tanto los t칤tulos originales como sus traducciones.

En la imagen de abajo se muestra el resultado de una ejecuci칩n realizada para mostrar las columnas "Title" y "Title_translated", para comprobar el correcto funcionamiento:
<div align="center">
  <img src="images/traduccion.png" alt="Gr치fica 1">
</div>

# RECUSROS UTILIZADOS

[1] https://www.youtube.com/watch?v=ATK6fm3cYfI
[2]https://github.com/ML4DS/ML4all/blob/master/B5_NLP/TM1.IntrodNLP/NLP_py3_NSF/notebook/NLPintro_professor.ipynb 
[3] [https://www.youtube.com/watch?v=scnxktbxYv8](https://www.youtube.com/watch?v=ZjaMvO3VEdU&t=360s)
[4]https://github.com/marvin2311/NN_Projects/blob/main/house_prices_ANN.ipynb 
[5]https://github.com/ML4DS/ML4all/blob/master/B2_regression/R2.kNN_Regression/regression_knn_professor.ipynb 
[6] 




