<div align="center">
  Proyecto Final
  
  Tratamiento de Datos
  
  M치ster de Ing. de Telecomunicaci칩n

  Daniel Mu침oz y Marina Rello
  
</div>

El proyecto b치sico consistir치 en la resoluci칩n de una tarea de regresi칩n, comparando las prestaciones obtenidas al utilizar distintas vectorizaciones de los documentos y al menos dos estrategias distintas de aprendizaje autom치tico, seg칰n se describe a continuaci칩n. Los pasos que debe seguir en su trabajo son los siguientes:

Como paso inicial, observamos el dataset con el que se va a trabajar:
<div align="center">
  <img src="images/Dataset_completo.png" alt="Gr치fica 1">
</div>

Se observan valores vac칤os en el dataset, por lo que se realiza una limpieza del mismo eliminando estos valores vac칤os.

Una vez eliminados los valores vac칤os del dataset, observamos los valores num칠ricos del dataset, con el fin de entender mejor la informaci칩n contenida:
<div align="center">
  <img src="images/outliers.png" alt="Gr치fica 1">
</div>

En cada histograma, los valores en el eje horizontal son extremadamente grandes, pero la mayor parte de los datos se concentran cerca de un rango m치s peque침o (cercano a cero). Esto sugiere que hay valores muy grandes (outliers) que "alargan" el eje y distorsionan la visualizaci칩n de la distribuci칩n principal. 

Se ha realizado una limpieza de dichos "outliers", tras la limpieza, volvemos a observamos los valores num칠ricos del dataset. Los valores extremos (outliers) que antes estiraban las escalas de los ejes han sido eliminados. Ahora las distribuciones muestran de forma m치s clara y representativa c칩mo se concentran los datos:

<div align="center">
  <img src="images/sin_outliers.png" alt="Gr치fica 1">
</div>


# 1. An치lisis de variables de entrada. Visualice la relaci칩n entre la variable de salida y algunas de las categor칤as en la variable categories y explique su potencial relevancia en el problema.

Como paso previo, se ha estudiado la relaci칩n de las distintas variables num칠ricas con el "rating":
<div align="center">
  <img src="images/numericas.png" alt="Gr치fica 1">
</div>

Se han realizado gr치ficos de dispersi칩n para analizar la relaci칩n entre las variables num칠ricas (fat, calories, protein y sodium) y el rating. Al observar los resultados, se puede concluir que no existe una relaci칩n clara o significativa, ya que los puntos se encuentran dispersos y no muestran patrones definidos. La variabilidad del "rating" se mantiene amplia para todos los valores de las variables num칠ricas, lo que indica que estas no tienen un impacto directo en el comportamiento del rating.


En el an치lisis de las variables de entrada, se ha explorado la relaci칩n entre la variable de salida rating y algunas categor칤as de la columna categories.
Se han ido probando distintas categor칤as de la variable categories para analizar su relaci칩n con la variable de salida rating, encontrando resultados contrastantes. Por ejemplo, las categor칤as "Pasta" y "Beef" muestran una mayor concentraci칩n de ratings en valores altos, especialmente entre 4 y 5, lo que sugiere una ligera relaci칩n positiva con el rating, ya que las recetas pertenecientes a estas categor칤as tienden a ser mejor valoradas. En cambio, otras categor칤as como "Alcoholic" y "Drink" presentan una distribuci칩n de ratings mucho m치s dispersa, con valores repartidos en todo el rango, lo que indica que no tienen una relaci칩n clara con la variable de salida.

<div align="center">
  <img src="images/categories_vs_rating.png" alt="Gr치fica 1">
</div>


# 2. Implementaci칩n de un pipeline para el preprocesado de los textos. Para esta tarea puede usar las librer칤as habituales (NLTK, Gensim o SpaCy), o cualquier otra librer칤a que considere oportuna. Tenga en cuenta que para trabajar con transformers el texto se pasa sin preprocesar.

En este paso se han transformado los datos de entrada de texto en bruto en una representaci칩n vectorial. Para ello, se ha eliminado la informaci칩n irrelevante de los datos de texto, preservando la mayor cantidad de informaci칩n relevante posible para capturar el contenido sem치ntico en la colecci칩n de documentos.
Para ello se han realizado los siguientes pasos:
  - Tokenization: Se ha dividido el texto en unidades m치s peque침as llamadas tokens, para poder trabajar con cada elemento del texto de manera independiente.
  - Homogeneization: Se estandariza el texto para reducir variaciones innecesarias, como convertir todo a min칰sculas, eliminar acentos y elementos no alfanum칠ricos o normalizar t칠rminos similares.
  - Cleaning: se han eliminado aquellas palabras que son muy comunes en el idioma y no aportan contenido sem치ntico 칰til 
  - Vectorization: Se ha transformado el texto procesado en una representaci칩n num칠rica (vectores) que los algoritmos pueden interpretar. Estos vectores capturan la informaci칩n sem치ntica y estructural del texto. Para ello, se ha creado un diccionario que asocia cada token con un identificador 칰nico y se han eliminado palabras que aparecen en muy pocos documentos o en demasiados. Cada documento se convierte en una lista de tuplas incluyendo el identificador 칰nico del token y la cantidad de veces que ese token aparece en el documento. Esto produce una representaci칩n dispersa (sparse vector), donde las palabras relevantes del texto est치n asociadas con su frecuencia. Finalmente, cada documento se representa como un vector disperso, donde los identificadores de los tokens corresponden a posiciones espec칤ficas del vector, y los valores representan la frecuencia.

A continuaci칩n, representamos los t칠rminos m치s frecuentes en el la columna descriptions:
<div align="center">
  <img src="images/token_distribution1.jpg" alt="Gr치fica 1" width="300">
  <img src="images/token_occurrence1.jpg" alt="Gr치fica 2" width="300">
</div>


# 3. Representaci칩n vectorial de los documentos mediante tres procedimientos diferentes:
## - TF-IDF
## - Word2Vec(es decir, la representaci칩n de los documentos como promedio de los embeddings de las palabras que lo forman)
## - Embeddings contextuales calculados a partir de modelos basados en transformers (e.g., BERT, RoBERTa, etc).

# 4. Entrenamiento y evaluaci칩n de modelos de regresi칩n utilizando al menos las dos estrategias siguientes de aprendizaje autom치tico:
Cada modelo de regresi칩n se ha entrenado y evaluado utilizando las tres t칠cnicas de vectorizaci칩n presentadas en el apartado anterior. Esto se ha realizado con el objetivo de comparar los resultados obtenidos y determinar cu치l de las t칠cnicas ofrece el mejor rendimiento.
## - Redes neuronales utilizando PyTorch para su implementaci칩n.
Se ha implementado una red neuronal dise침ada con dos capas ocultas: la primera con 50 neuronas y la segunda con 25, ambas utilizando la funci칩n de activaci칩n ReLU, y una capa de salida con una 칰nica neurona para predecir valores continuos. Se divide el conjunto de datos en entrenamiento (80%) y prueba (20%), convirtiendo los datos a tensores para su uso en PyTorch. La red se entrena durante 25 칠pocas usando el optimizador Adam y la funci칩n de p칠rdida de error cuadr치tico medio (MSELoss), procesando los datos en lotes de 32 muestras con un DataLoader para mejorar la eficiencia. Finalmente, se eval칰a el modelo en los datos de prueba calculando la p칠rdida y el coeficiente R2.

## - Al menos otra t칠cnica implementada en la librer칤a Scikit-learn (e.g., K-NN, SVM, Random Forest, etc)
Se han implementado los modelos de regresi칩n k-NN y Random Forest. 
Para el modelo k-NN, se ha realizado validaci칩n cruzada en cada vectorizaci칩n con el objetivo de determinar el valor 칩ptimo del hiperpar치metro 洧녲. 
En el caso de Random Forest, debido a las limitaciones de tiempo y al elevado costo computacional, el ajuste de los hiperpar치metros "n_estimators" y "max_depth" se ha realizado 칰nicamente para una de las vectorizaciones, utilizando los valores obtenidos como referencia para el resto de los modelos.

A continuaci칩n, se presenta una tabla comparativa que recoge los resultados obtenidos para cada modelo de regresi칩n aplicado a las diferentes t칠cnicas de vectorizaci칩n.

<div align="center">
<table>
    <tr>
        <th>Modelo</th>
        <th>Vectorizaci칩n</th>
        <th>MSE</th>
        <th>MAE</th>
        <th>R</th>
        <th>RMSE</th>
    </tr>
    <tr>
        <td rowspan="3">k-NN</td>
        <td>TF-IDF</td>
        <td>1.436</td>
        <td>0.807</td>
        <td>1.198</td>
        <td>0.069</td>
    </tr>
    <tr>
        <td>Word2Vec</td>
        <td>1.518</td>
        <td>0.822</td>
        <td>1.232</td>
        <td>0.016</td>
    </tr>
    <tr>
        <td>BERT</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
    </tr>
    <tr>
        <td rowspan="3">Random Forest</td>
        <td>TF-IDF</td>
        <td>1.387</td>
        <td>0.794</td>
        <td>1.178</td>
        <td>0.101/td>
    </tr>
    <tr>
        <td>Word2Vec</td>
        <td>1.413</td>
        <td>0.801</td>
        <td>1.189</td>
        <td>0.084</td>
    </tr>
    <tr>
        <td>BERT</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
        <td>?</td>
    </tr>
    <tr>
        <td rowspan="3">Red Neuronal</td>
        <td>TF-IDF</td>
        <td>1.524</td>
        <td>X</td>
        <td>0.043</td>
        <td>X3</td>
    </tr>
    <tr>
        <td>Word2Vec</td>
        <td>1.585</td>
        <td>X</td>
        <td>0.004</td>
        <tdX</td>
    </tr>
    <tr>
        <td>BERT</td>
        <td>?</td>
        <td>X</td>
        <td>?</td>
        <td>X</td>
    </tr>
</table>

</div>



# 5. Comparaci칩n de lo obtenido en el paso 3 con el fine-tuning de un modelo preentrenado con Hugging Face. En este paso se pide utilizar un modelo de tipo transformer con una cabeza dedicada a la tarea de regresi칩n.



