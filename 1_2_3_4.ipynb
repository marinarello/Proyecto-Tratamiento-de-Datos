{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\munoz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\munoz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\munoz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\munoz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\munoz\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#IMPORTS NECESARIOS PARA EL PROYECTO\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tqdm\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, r2_score as r2\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch import optim\n",
        "from sklearn.metrics import r2_score\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.matutils import corpus2dense\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.sparse import hstack\n",
        "from nltk import download\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsRegressor  # Import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV  # Import GridSearchCV\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "# Descargar los recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDuSLrs8Oatl",
        "outputId": "813bfb15-f61f-46c8-9a72-98d97b969beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          directions   fat  \\\n",
            "0  [1. Place the stock, lentils, celery, carrot, ...   7.0   \n",
            "1  [Combine first 9 ingredients in heavy medium s...  23.0   \n",
            "2  [In a large heavy saucepan cook diced fennel a...   7.0   \n",
            "3  [Heat oil in heavy large skillet over medium-h...   NaN   \n",
            "4  [Preheat oven to 350°F. Lightly grease 8x8x2-i...  32.0   \n",
            "\n",
            "                       date  \\\n",
            "0 2006-09-01 04:00:00+00:00   \n",
            "1 2004-08-20 04:00:00+00:00   \n",
            "2 2004-08-20 04:00:00+00:00   \n",
            "3 2009-03-27 04:00:00+00:00   \n",
            "4 2004-08-20 04:00:00+00:00   \n",
            "\n",
            "                                          categories  calories  \\\n",
            "0  [Sandwich, Bean, Fruit, Tomato, turkey, Vegeta...     426.0   \n",
            "1  [Food Processor, Onion, Pork, Bake, Bastille D...     403.0   \n",
            "2  [Soup/Stew, Dairy, Potato, Vegetable, Fennel, ...     165.0   \n",
            "3  [Fish, Olive, Tomato, Sauté, Low Fat, Low Cal,...       NaN   \n",
            "4  [Cheese, Dairy, Pasta, Vegetable, Side, Bake, ...     547.0   \n",
            "\n",
            "                                                desc  protein  rating  \\\n",
            "0                                               None     30.0   2.500   \n",
            "1  This uses the same ingredients found in boudin...     18.0   4.375   \n",
            "2                                               None      6.0   3.750   \n",
            "3  The Sicilian-style tomato sauce has tons of Me...      NaN   5.000   \n",
            "4                                               None     20.0   3.125   \n",
            "\n",
            "                                         title  \\\n",
            "0              Lentil, Apple, and Turkey Wrap    \n",
            "1  Boudin Blanc Terrine with Red Onion Confit    \n",
            "2                Potato and Fennel Soup Hodge    \n",
            "3             Mahi-Mahi in Tomato Olive Sauce    \n",
            "4                    Spinach Noodle Casserole    \n",
            "\n",
            "                                         ingredients  sodium  \n",
            "0  [4 cups low-sodium vegetable or chicken stock,...   559.0  \n",
            "1  [1 1/2 cups whipping cream, 2 medium onions, c...  1439.0  \n",
            "2  [1 fennel bulb (sometimes called anise), stalk...   165.0  \n",
            "3  [2 tablespoons extra-virgin olive oil, 1 cup c...     NaN  \n",
            "4  [1 12-ounce package frozen spinach soufflé, th...   452.0  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Carga del archivo JSON\n",
        "fichero = \"C:/Users/munoz/OneDrive/Desktop/UC3M/DATOS/full_format_recipes.json\"\n",
        "data = pd.read_json(fichero)\n",
        "\n",
        "# Muestra las primeras filas\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iLLL7x0OdLX",
        "outputId": "96081deb-4444-4c9b-a7e4-69f1909aa1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columna 'fat': Límites [-43.00, 87.00]\n",
            "Filas restantes después de limpiar 'fat': 10077\n",
            "Columna 'calories': Límites [-520.00, 1280.00]\n",
            "Filas restantes después de limpiar 'calories': 9917\n",
            "Columna 'protein': Límites [-39.00, 66.00]\n",
            "Filas restantes después de limpiar 'protein': 9584\n",
            "Columna 'rating' ha sido excluida del proceso.\n",
            "Columna 'sodium': Límites [-1051.00, 1784.00]\n",
            "Filas restantes después de limpiar 'sodium': 9190\n",
            "Tamaño original del dataset: (10608, 11)\n",
            "Tamaño después de limpiar outliers: (9190, 11)\n"
          ]
        }
      ],
      "source": [
        "############################ LIMPIO EL DATAFRAME #########################\n",
        "data.replace(r'^\\s*$', np.nan, regex=True, inplace=True)  # Reemplazar cadenas vacías por NaN\n",
        "data_clean = data.dropna()\n",
        "# Seleccionar solo las columnas numéricas\n",
        "data_numeric = data_clean.select_dtypes(include=['number'])\n",
        "\n",
        "data_cleaned = data_clean.copy()\n",
        "\n",
        "exclude_column = 'rating'\n",
        "# Iterar columna por columna y eliminar outliers usando IQR\n",
        "for column in data_numeric.columns:\n",
        "    if column == exclude_column:  \n",
        "        print(f\"Columna '{column}' ha sido excluida del proceso.\")\n",
        "        continue\n",
        "    Q1 = data_cleaned[column].quantile(0.25)  # Primer cuartil\n",
        "    Q3 = data_cleaned[column].quantile(0.75)  # Tercer cuartil\n",
        "    IQR = Q3 - Q1                             # Rango intercuartílico\n",
        "\n",
        "    # Definir los límites\n",
        "    lower_bound = Q1 - 2 * IQR\n",
        "    upper_bound = Q3 + 2 * IQR\n",
        "\n",
        "    # Filtrar filas dentro de los límites para la columna actual\n",
        "    data_cleaned = data_cleaned[\n",
        "        (data_cleaned[column] >= lower_bound) & (data_cleaned[column] <= upper_bound)\n",
        "    ]\n",
        "    print(f\"Columna '{column}': Límites [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "    print(f\"Filas restantes después de limpiar '{column}': {data_cleaned.shape[0]}\")\n",
        "\n",
        "print(\"Tamaño original del dataset:\", data_clean.shape)\n",
        "print(\"Tamaño después de limpiar outliers:\", data_cleaned.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTRHmWCUjL5H",
        "outputId": "38d13cba-14fe-4a4f-c2a2-14a9593b9022"
      },
      "outputs": [],
      "source": [
        "##################################### HUGGING FACE ##############################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1MPcSOoww4Hk",
        "outputId": "f253c693-d64e-4909-c680-01136333852d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='243' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [243/460 1:33:58 < 1:24:37, 0.04 it/s, Epoch 0.53/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>13.165600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.607500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.220800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.596800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.495800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.539200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.552300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.999400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.133100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.984300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.583100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.913400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.787100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.485200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.478900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.778600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.812800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.952400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.264900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [460/460 2:57:58, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>13.165600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.607500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.220800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.596800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.495800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.539200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.552300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.999400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.133100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.984300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.583100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.913400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.787100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.485200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.478900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.778600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.812800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.952400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.264900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.612100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.596300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.399600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.508400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.856900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.858600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.362600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.918200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.770200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.635400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.608200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.143100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.427300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.250200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.879700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.299600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.495300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.166300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29/29 13:09]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./fine-tuned-bert/tokenizer_config.json',\n",
              " './fine-tuned-bert/special_tokens_map.json',\n",
              " './fine-tuned-bert/vocab.txt',\n",
              " './fine-tuned-bert/added_tokens.json')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from datasets import DatasetDict\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "texts = data_cleaned['desc']  # Usar 'desc' del dataframe limpio como entrada\n",
        "labels = data_cleaned['rating']  # Usar 'rating' del dataframe limpio como salida\n",
        "\n",
        "#Split de la entrada y salida en train y test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "#BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "#DATASET\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, train_labels)\n",
        "test_dataset = Dataset(test_encodings, test_labels)\n",
        "\n",
        "\n",
        "#ENTRENAMIENTO \n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          \n",
        "    num_train_epochs=1,              \n",
        "    per_device_train_batch_size=16,  \n",
        "    per_device_eval_batch_size=64,   \n",
        "    warmup_steps = 10,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir = './logs',\n",
        "    logging_steps=10,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# EVALUACION\n",
        "trainer.evaluate()\n",
        "model.save_pretrained('./fine-tuned-bert')\n",
        "tokenizer.save_pretrained('./fine-tuned-bert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from datasets import DatasetDict\n",
        "from datasets import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###########################################  BERT ###############################################\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Cargar BERT\n",
        "\n",
        "model = BertModel.from_pretrained('./fine-tuned-bert',output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('./fine-tuned-bert')\n",
        "\n",
        "def tokenize_BERT(text_list):\n",
        "\n",
        "  embeddings = []\n",
        "  for idx, text in enumerate(text_list):\n",
        "    if isinstance(text, list):\n",
        "            text = \" \".join(text)  \n",
        "\n",
        "    # Colocar tokens especiales de BERT\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "   \n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "    if len(tokenized_text) > 512:\n",
        "            tokenized_text = tokenized_text[:512]  \n",
        "\n",
        "    # Mapear los tokens a sus ids de BERT\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segment_id = idx % 2  # Diferenciar frases\n",
        "    segments_ids = [segment_id] * len(tokenized_text)\n",
        "\n",
        "    # Creamos tensores \n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensor = torch.tensor([segments_ids])\n",
        "\n",
        "    # Generar embeddings  BERT\n",
        "    with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, segments_tensor)\n",
        "      hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    # Promediar los tokens para obtener el embedding del texto\n",
        "    text_embedding = torch.mean(hidden_states, dim=1).squeeze().tolist()\n",
        "    embeddings.append(text_embedding)\n",
        "\n",
        "  return embeddings\n",
        "\n",
        "direcciones = data_cleaned['directions'].tolist()\n",
        "categorias = data_cleaned['categories'].tolist()\n",
        "descripciones = data_cleaned['desc'].tolist()\n",
        "titulo = data_cleaned['title'].tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directions_embeddings = tokenize_BERT(direcciones)\n",
        "\n",
        "data_cleaned['directions_embeddings'] = directions_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "categoriess_embeddings = tokenize_BERT(categorias)\n",
        "\n",
        "data_cleaned['categoriess_embeddings'] = categoriess_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "descriptions_embeddings = tokenize_BERT(descripciones)\n",
        "\n",
        "data_cleaned['descriptions_embeddings'] = descriptions_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "title_embeddings = tokenize_BERT(titulo)\n",
        "\n",
        "data_cleaned['title_embeddings'] = title_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_BERT = np.hstack([directions_embeddings, categoriess_embeddings, descriptions_embeddings, title_embeddings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.5221141576766968\n",
            "R2: 0.04408430935549723\n"
          ]
        }
      ],
      "source": [
        "#Red Neuronal\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, in_count):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc =nn.Sequential(\n",
        "            nn.Linear(in_count, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_BERT, data_cleaned['rating'], test_size=0.20, random_state=42)\n",
        "\n",
        "Xtrain_tensor= torch.tensor(Xtrain, dtype=torch.float32)\n",
        "Ytrain_tensor= torch.tensor(Ytrain.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "Xtest_tensor= torch.tensor(Xtest, dtype=torch.float32)\n",
        "Ytest_tensor= torch.tensor(Ytest.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "dataset_train = TensorDataset(Xtrain_tensor, Ytrain_tensor)\n",
        "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model = NN(Xtrain.shape[1])\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(25):\n",
        "  for Xbatch, Ybatch in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    Ybatch_pred=model(Xbatch)\n",
        "    loss = loss_fn(Ybatch_pred, Ybatch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  Ytest_pred=model(Xtest_tensor)\n",
        "  loss_test = loss_fn(Ytest_pred, Ytest_tensor)\n",
        "  print(\"Test Loss:\", loss_test.item())\n",
        "  Ytest_pred_array=Ytest_pred.numpy()\n",
        "  valor_r2=r2(Ytest,Ytest_pred_array)\n",
        "  print(\"R2:\", valor_r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor valor de k: 57\n",
            "MSE promedio para k=57: 1.587287980299239\n",
            "MSE:  1.4826781664105122\n",
            "MAE:  0.7781508313765916\n",
            "RMSE:  1.2176527281661682\n",
            "R2:  0.0394459174279439\n"
          ]
        }
      ],
      "source": [
        "#KNN\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_BERT, data_cleaned['rating'], test_size=0.30, random_state=42)\n",
        "X_train_t, X_val, Y_train_t, Y_val = train_test_split(X_train, Y_train, test_size=0.30, random_state=42)\n",
        "\n",
        "#Realizamos cross validation para encontrar el mejor valor del hiperparámetro k:\n",
        "param_grid = {'n_neighbors': range(40, 70)}\n",
        "\n",
        "knn_model = KNeighborsRegressor()\n",
        "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, scoring=scorer, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_val, Y_val)\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "best_score = -grid_search.best_score_\n",
        "\n",
        "print(f\"Mejor valor de k: {best_k}\")\n",
        "print(f\"MSE promedio para k={best_k}: {best_score}\")\n",
        "\n",
        "#realizo regresión:\n",
        "k = best_k    # Number of neighbors\n",
        "n_points = 1000\n",
        "\n",
        "my_knn = KNeighborsRegressor(n_neighbors=k)\n",
        "my_knn.fit(X_train_t,Y_train_t)\n",
        "\n",
        "Y_predict = my_knn.predict(X_test)\n",
        "\n",
        "\n",
        "MSE = mse(Y_test, Y_predict)\n",
        "MAE = mae(Y_test, Y_predict)\n",
        "RMSE = np.sqrt(MSE)\n",
        "R2 = r2(Y_test, Y_predict)\n",
        "\n",
        "print(\"MSE: \", MSE)\n",
        "print(\"MAE: \", MAE)\n",
        "print(\"RMSE: \", RMSE)\n",
        "print(\"R2: \", R2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE:  1.3988340706618196\n",
            "MAE:  0.7936792316451121\n",
            "RMSE:  1.182723158926813\n",
            "R2:  0.09376437324357423\n"
          ]
        }
      ],
      "source": [
        "#Random Forest\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_BERT, data_cleaned['rating'], test_size=0.30, random_state=42)\n",
        "X_train_t, X_val, Y_train_t, Y_val = train_test_split(X_train, Y_train, test_size=0.30, random_state=42)\n",
        "\n",
        "#Utilizamos hiperparámetros calculados antes\n",
        "\n",
        "# Modelo 1: Random Forest\n",
        "model_rf = RandomForestRegressor(n_estimators=500, max_depth = 10, random_state=42)\n",
        "model_rf.fit(X_train_t, Y_train_t)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_rf = model_rf.predict(X_test)  # Predicciones con Random Forest\n",
        "\n",
        "\n",
        "MSE = mse(Y_test, y_pred_rf)\n",
        "MAE = mae(Y_test, y_pred_rf)\n",
        "RMSE = np.sqrt(MSE)\n",
        "R2 = r2(Y_test, y_pred_rf)\n",
        "\n",
        "print(\"MSE: \", MSE)\n",
        "print(\"MAE: \", MAE)\n",
        "print(\"RMSE: \", RMSE)\n",
        "print(\"R2: \", R2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
